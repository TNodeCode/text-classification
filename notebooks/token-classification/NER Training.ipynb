{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13df4d4e",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import defaultdict\n",
    "from datasets import DatasetDict, get_dataset_config_names, load_dataset\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "from transformers import AutoConfig, AutoTokenizer, Trainer, TrainingArguments, XLMRobertaConfig, DataCollatorForTokenClassification\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel, RobertaPreTrainedModel\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a0491",
   "metadata": {},
   "source": [
    "## Build the dataset\n",
    "\n",
    "We will create a NER dataset containing German, French, English and Italian language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee518d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtreme_subsets = get_dataset_config_names(\"xtreme\")\n",
    "print(f\"The xtreme dataset has {len(xtreme_subsets)} subsets\")\n",
    "\n",
    "panx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\n",
    "print(f\"There are {len(panx_subsets)} PAN-X subsets\")\n",
    "\n",
    "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
    "fracs = [0.629, 0.229, 0.084, 0.059]\n",
    "\n",
    "panx_ch = defaultdict(DatasetDict)\n",
    "\n",
    "for lang, frac in zip(langs, fracs):\n",
    "    # Load corpus for one language\n",
    "    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
    "    # shuffle and downsample each split according to spoken proportion\n",
    "    for split in ds:\n",
    "        panx_ch[lang][split] = (\n",
    "            ds[split].shuffle(seed=0).select(range(int(frac * ds[split].num_rows)))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42814747",
   "metadata": {},
   "source": [
    "## Dataset Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf114c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "element = panx_ch[\"de\"][\"train\"][0]\n",
    "element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566cba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in panx_ch[\"de\"][\"train\"].features.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77332431",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea74404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag_names(batch):\n",
    "    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n",
    "\n",
    "panx_de = panx_ch[\"de\"].map(create_tag_names)\n",
    "panx_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a29427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_example = panx_de[\"train\"][0]\n",
    "pd.DataFrame([de_example[\"tokens\"], de_example[\"ner_tags_str\"]], [\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc354bf3",
   "metadata": {},
   "source": [
    "## Token Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb233eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
    "    config_class = XLMRobertaConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        # load roberta feature extractor\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        # model head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(config.hidden_dropout_prob),\n",
    "            nn.Linear(config.hidden_size, config.num_labels)\n",
    "        )\n",
    "        # load and initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
    "        # extract features\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        # send features through classification head\n",
    "        logits = self.head(outputs.last_hidden_state)\n",
    "        # calculate loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        # return model output\n",
    "        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001000e7",
   "metadata": {},
   "source": [
    "## Text Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a3dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
    "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b00a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = \"bert-base-cased\"\n",
    "xlmr_model_name = \"xlm-roberta-base\"\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)\n",
    "\n",
    "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name, num_labels=tags.num_classes, label2id=tag2index, id2label=index2tag)\n",
    "xlmr_model = XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a2166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text, tags, model, tokenizer):\n",
    "    # Get the tokens\n",
    "    tokens = tokenizer(text).tokens()\n",
    "    # Encode seqeunce into IDs\n",
    "    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "    # Get predictions as distribution over 7 possible classes\n",
    "    outputs = model(input_ids)[0]\n",
    "    # Take argmax to get most likely class per token\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    # Convert to DataFrame\n",
    "    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
    "    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905acbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, labels = de_example[\"tokens\"], de_example[\"ner_tags\"]\n",
    "words, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2da332",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = xlmr_tokenizer(de_example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "word_ids = tokenized_input.word_ids()\n",
    "pd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946c8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_word_idx = None\n",
    "label_ids = []\n",
    "\n",
    "for word_idx in word_ids:\n",
    "    # mask all special characters and double NER tags\n",
    "    if word_idx is None or word_idx == previous_word_idx:\n",
    "        label_ids.append(-100) # -100 is the value of ignore_index in PyTorch's CELoss function\n",
    "    elif word_idx != previous_word_idx:\n",
    "        label_ids.append(labels[word_idx])\n",
    "    previous_word_idx = word_idx\n",
    "\n",
    "labels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\n",
    "index = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n",
    "\n",
    "pd.DataFrame([tokens, word_ids, label_ids, labels], index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b9df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for idx, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def encode_panx_dataset(corpus):\n",
    "    return corpus.map(tokenize_and_align_labels, batched=True, remove_columns=[\"langs\", \"ner_tags\", \"tokens\"])\n",
    "\n",
    "panx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843b5834",
   "metadata": {},
   "source": [
    "## Performance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc33e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"O\", \"B-PER\", \"I-PER\", \"O\"], [\"B-PER\", \"I-PER\", \"O\"]]\n",
    "y_pred = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"B-PER\", \"I-PER\", \"O\"], [\"B-PER\", \"O\", \"I-PER\"]]\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a9d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_predictions(predictions, label_ids):\n",
    "    \"\"\"Convert mdoel outputs into seqeval format.\"\"\"\n",
    "    # First get the most likely IDs\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    batch_size, seq_len = preds.shape\n",
    "    # these are the two lists seqeval expects\n",
    "    labels_list, preds_list = [], []\n",
    "\n",
    "    for batch_idx in range(batch_size): # iterate over batches\n",
    "        example_labels, example_preds = [], []\n",
    "        for seq_idx in range(seq_len): # iterate over sequence IDs\n",
    "            # ignore label IDs = -100\n",
    "            if label_ids[batch_idx, seq_idx] != -100:\n",
    "                # map IDs to tags\n",
    "                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n",
    "                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n",
    "            labels_list.append(example_labels)\n",
    "            preds_list.append(example_preds)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute metrics during training.\"\"\"\n",
    "    y_pred, y_true = align_predictions(eval_pred.predictions, eval_pred.label_ids)\n",
    "    return {\"f1\", f1_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45753375",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f8fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "batch_size = 4\n",
    "logging_steps = len(panx_de_encoded[\"train\"]) // batch_size\n",
    "model_name = f\"{xlmr_model_name}-finetuned-panx-de\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = f\"../../checkpoints/{model_name}\",\n",
    "    log_level=\"error\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_steps=1e6,\n",
    "    weight_decay=0.01,\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4f0c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data collator pads each input sequence to the largest sequence length in a batch\n",
    "data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)\n",
    "\n",
    "def model_init():\n",
    "    \"\"\"Initialize a new model.\"\"\"\n",
    "    return XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfac2035",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=panx_de_encoded[\"train\"],\n",
    "    eval_dataset=panx_de_encoded[\"validation\"],\n",
    "    tokenizer=xlmr_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7739143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8475c901",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2706bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\n",
    "tag_text(text_de, tags, trainer.model, xlmr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b273d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
